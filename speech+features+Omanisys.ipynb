{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Speech Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute MFCC features from an audio signal.\n",
    "\n",
    "Parameters:\t\n",
    "signal – the audio signal from which to compute features. Should be an N*1 array\n",
    "\n",
    "samplerate – the samplerate of the signal we are working with.\n",
    "\n",
    "winlen – the length of the analysis window in seconds. Default is 0.025s (25 milliseconds)\n",
    "\n",
    "winstep – the step between successive windows in seconds. Default is 0.01s (10 milliseconds)\n",
    "\n",
    "numcep – the number of cepstrum to return, default 13\n",
    "\n",
    "nfilt – the number of filters in the filterbank, default 26.\n",
    "\n",
    "nfft – the FFT size. Default is 512.\n",
    "\n",
    "lowfreq – lowest band edge of mel filters. In Hz, default is 0.\n",
    "\n",
    "highfreq – highest band edge of mel filters. In Hz, default is samplerate/2\n",
    "\n",
    "preemph – apply preemphasis filter with preemph as coefficient. 0 is no filter. Default is 0.97.\n",
    "\n",
    "ceplifter – apply a lifter to final cepstral coefficients. 0 is no lifter. Default is 22.\n",
    "\n",
    "appendEnergy – if this is true, the zeroth cepstral coefficient is replaced with the log of the total frame energy.\n",
    "\n",
    "winfunc – the analysis window to apply to each frame. By default no window is applied. You can use numpy window functions here e.g. winfunc=numpy.hamming\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Returns:\t\n",
    "A numpy array of size (NUMFRAMES by numcep) containing features. Each row holds 1 feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features from speech (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This code is being used to extract features from audio files and save those features as an nxm array\n",
    "#Speech features are extracted using library from: https://github.com/jameslyons/python_speech_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from python_speech_features import mfcc, fbank, logfbank, ssc\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "from python_speech_features import mfcc\n",
    "from python_speech_features import logfbank\n",
    "import scipy.io.wavfile as wav\n",
    "import glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import pandas as pd\n",
    "# from features.base import ssc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cepCount=13 #no of MFCC coefficients\n",
    "nfeatures = 7 #features per coefficient\n",
    "elcount = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audioread(datafs,gender_flag):\n",
    "    (data, fs) = wav.read (\"E:\\Research Project\\srp\\speech_emotion_recognition-master/train_sounds/*.wav\")#(datafs)\n",
    "    ceps = mfcc(fs,numcep=cepCount)\n",
    "    feat2 = ssc(fs,samplerate=16000,winlen=0.025,winstep=0.01,\n",
    "          nfilt=26,nfft=512,lowfreq=0,highfreq=None,preemph=0.97)\n",
    "    \n",
    "    ls = []\n",
    "    for i in range(ceps.shape[1]):\n",
    "        temp = ceps[:,i]\n",
    "        dtemp = np.gradient(temp)\n",
    "        lfeatures  = [np.mean(temp), np.var(temp), np.amax(temp), np.amin(temp), \n",
    "                      np.var(dtemp), np.mean(temp[0:temp.shape[0]/2]), np.mean(temp[temp.shape[0]/2:temp.shape[0]])]\n",
    "        temp2 = np.array(lfeatures)\n",
    "        ls.append(temp2)\n",
    "        ls2 = []\n",
    "\n",
    "    for i in range(feat2.shape[1]):\n",
    "        temp = feat2[:,i]\n",
    "        dtemp = np.gradient(temp)\n",
    "        lfeatures = [np.mean(temp), np.var(temp), np.amax(temp), np.amin(temp), \n",
    "                     np.var(dtemp), np.mean(temp[0:temp.shape[0]/2]), np.mean(temp[temp.shape[0]/2:temp.shape[0]])]\n",
    "        temp2 = np.array(lfeatures)\n",
    "        ls2.append(temp2)\n",
    "        source = np.array(ls).flatten()\n",
    "    source = np.append(source, np.array(ls2).flatten())\n",
    "    return source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    emotions = ['anger','disgust','fear','happy','neutral','sadness','sarcastic','surprise']\n",
    "\n",
    "    female_path = 'E:\\Research Project\\srp\\speech_emotion_recognition-master/train_sounds/*.wav'\n",
    "    \n",
    "    max_len_male=max_len_female=0\n",
    "\n",
    "    X_female=np.empty(shape=(1200,(cepCount + 26)*nfeatures ))\n",
    "\n",
    "    y_female=np.empty(1200)\n",
    "    mcount=fcount=0\n",
    "    print (\"Loop Started....\")\n",
    "#filename style: 3.2.anger-01.wav\n",
    "\n",
    "    for j in range (1,16):\n",
    "        if(j<=9):\n",
    "            jstring = '0' + str(j)\n",
    "        else:\n",
    "            jstring = str(j)\n",
    "    for i in range(1,11):\n",
    "\n",
    "            for emo in emotions:\n",
    "                x = female_path+str(i) + '/' + emo + '/' + '4.' + str(i) + '.' +emo +'-' + str(jstring) + '.wav'\n",
    "                X_female[fcount]=audioread(x,gender_flag='female')\n",
    "                y_female[fcount]=emotions.index(emo)\n",
    "                fcount+=1\n",
    "            return X_female, y_female\n",
    "        \n",
    "    if __name__ == '__main__':\n",
    "        print (\"Start loading...\")\n",
    "        X_female, y_female =load_data('E:\\Research Project\\srp\\speech_emotion_recognition-master/train_sounds/*.wav')\n",
    "        print (\"Start dumping...\")\n",
    "\n",
    "        X_female.dump(\"X_female.dat\")\n",
    "\n",
    "        y_female.dump(\"y_female.dat\")\n",
    "        print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extracting features from speech (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature extraction is based on python_speech_features\n",
    "\n",
    "given on it's documentation website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from python_speech_features import mfcc\n",
    "from python_speech_features import logfbank\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "(rate,sig) = wav.read('E:\\Research Project\\srp\\speech_emotion_recognition-master/train_sounds/*.wav')  #(\"E:\\Research Project\\Speech_data_set\\sad.wav\")\n",
    "mfcc_feat = mfcc(sig,rate)\n",
    "fbank_feat = logfbank(sig,rate)\n",
    "\n",
    "print(fbank_feat[1:3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:frame length (610) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n",
      "WARNING:root:frame length (610) is greater than FFT size (512), frame will be truncated. Increase NFFT to avoid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.98925002  1.55628669  1.02290869  1.31898838  0.10848723 -1.21632936\n",
      "   0.22212411  0.26025452  0.73097947  0.67905076  2.23369275  2.50297406\n",
      "   0.98005822  2.11773165  2.61645334  2.27871831  2.91823917  3.70747811\n",
      "   3.88890932  4.18140709  3.71037332  4.37614944  4.48221616  4.62118497\n",
      "   5.01414063  4.97182261]\n",
      " [ 4.83645032  3.22506597  2.26641494  1.21201116  0.65638843  0.51337736\n",
      "   0.87283654  1.30207716  0.88341416  1.69492847  4.42419862  5.53619654\n",
      "   4.99265266  4.72749986  3.9100239   4.20551209  7.06144029  8.97861143\n",
      "   7.29488082  4.82054291  5.81333404  5.96198238  5.51754492  5.53133111\n",
      "   5.20259873  5.44707537]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from python_speech_features import mfcc\n",
    "from python_speech_features import logfbank\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "(rate,sig) = wav.read(\"E:\\Research Project\\Speech_data_set\\happy.wav\")\n",
    "mfcc_feat = mfcc(sig,rate)\n",
    "fbank_feat = logfbank(sig,rate)\n",
    "\n",
    "print(fbank_feat[1:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
